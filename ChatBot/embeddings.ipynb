{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargamos variables de entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "result = load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "if result:\n",
    "    print(\"Variables de entorno cargadas exitosamente.\")\n",
    "else:\n",
    "    print(\"No se pudo cargar el archivo .env.\")\n",
    "\n",
    "    # Crear el archivo .env con las variables deseadas si no existe\n",
    "    with open(\".env\", \"w\") as f:\n",
    "        f.write(f\"OPENAI_API_KEY=MY_OPENAI_API_KEY\\n\")\n",
    "        f.write(f\"PINECONE_API_KEY=MY_PINECONE_API_KEY\\n\")\n",
    "        f.write(f\"PINECONE_ENVIRONMENT=MY_PINECONE_ENVIRONMENT\\n\")\n",
    "\n",
    "    print(\"Se creó el archivo .env con las variables iniciales.\")\n",
    "    # Vuelve a cargar las variables de entorno después de crear el archivo\n",
    "    load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\") or \"MY_OPENAI_API_KEY\"\n",
    "os.environ['PINECONE_API_KEY'] =  os.getenv(\"PINECONE_API_KEY\") or \"MY_PINECONE_API_KEY\"\n",
    "os.environ[\"PINECONE_ENVIRONMENT\"] = os.getenv(\"PINECONE_ENVIRONMENT\") or \"MY_PINECONE_ENVIRONMENT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformacion de codificacion de .txt (Utf-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ftfy\n",
    "\n",
    "def fix_encoding(text):\n",
    "    # Utilizar ftfy para corregir la codificación del texto\n",
    "    fixed_text = ftfy.fix_text(text)\n",
    "    return fixed_text\n",
    "\n",
    "def convert_to_utf8(text, new_file_path):\n",
    "    with open(new_file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "\n",
    "    print(f\"El texto ha sido convertido a UTF-8 y guardado en '{new_file_path}'.\")\n",
    "\n",
    "def split_document(document):\n",
    "    # Dividir el documento en párrafos usando el punto final como delimitador\n",
    "    paragraphs = document.split(\".\")\n",
    "    return paragraphs\n",
    "\n",
    "def process_fragment(fragment):\n",
    "    # Procesar cada párrafo\n",
    "    # En este ejemplo, simplemente eliminaremos los caracteres especiales, convertiremos el texto a minúsculas\n",
    "    # y eliminaremos los saltos de línea\n",
    "    processed_fragment = fragment.lower()  # Convertir a minúsculas\n",
    "    processed_fragment = ''.join(filter(str.isalnum, processed_fragment))  # Eliminar caracteres no alfanuméricos\n",
    "    processed_fragment = processed_fragment.replace('\\n', '')  # Eliminar saltos de línea\n",
    "    return processed_fragment\n",
    "\n",
    "def convert_directory_to_utf8(input_directory, output_directory):\n",
    "    # Asegúrate de que el directorio de salida exista\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Procesar cada archivo en el directorio de entrada\n",
    "    for filename in os.listdir(input_directory):\n",
    "        file_path = os.path.join(input_directory, filename)\n",
    "        new_file_path = os.path.join(output_directory, filename)\n",
    "        if os.path.isfile(file_path) and filename.endswith(\".txt\"):\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    document = f.read()\n",
    "            except UnicodeDecodeError:\n",
    "                # Si la codificación utf-8 falla, intenta con latin-1\n",
    "                with open(file_path, 'r', encoding='latin-1') as f:\n",
    "                    document = f.read()\n",
    "\n",
    "            paragraphs = split_document(document)\n",
    "            processed_document = ''\n",
    "            for paragraph in paragraphs:\n",
    "                processed_paragraph = process_fragment(paragraph)\n",
    "                processed_document += processed_paragraph\n",
    "\n",
    "            convert_to_utf8(document, new_file_path)\n",
    "\n",
    "# Directorios de entrada y salida\n",
    "input_directory = \"./../TXT_no_UTF8\"\n",
    "output_directory = \"./../TXT_UTF8\"\n",
    "\n",
    "# Convertir todos los archivos de texto en el directorio de entrada a UTF-8\n",
    "convert_directory_to_utf8(input_directory, output_directory)\n",
    "\n",
    "print(f\"Todos los archivos en '{input_directory}' han sido convertidos a UTF-8 y guardados en '{output_directory}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creacion de Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargar librerias para cargar los documentos\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  documents_llm_model(output_directory):\n",
    "    all_documents = []\n",
    "\n",
    "    # Procesar cada archivo en el directorio\n",
    "    for filename in os.listdir(output_directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(output_directory, filename)\n",
    "            \n",
    "            # Crear un TextLoader para cargar el archivo\n",
    "            loader = TextLoader(file_path, autodetect_encoding=True)\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Dividir los documentos en fragmentos de texto con un tamaño máximo de 800 tokens\n",
    "            text_splitter = CharacterTextSplitter(chunk_size=10, chunk_overlap=0)\n",
    "            docs = text_splitter.split_documents(documents)\n",
    "            \n",
    "            # Agregar los documentos procesados a la lista general\n",
    "            all_documents.extend(docs)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    # Create vectors\n",
    "    vectorstore = FAISS.from_documents(all_documents, embeddings)  # Usar all_documents en lugar de docs\n",
    "    # Persist the vectors locally on disk\n",
    "    vectorstore.save_local(\"faiss_index\")\n",
    "\n",
    "documents_llm_model(output_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot-env",
   "language": "python",
   "name": "chatbot-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
